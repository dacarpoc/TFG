{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Modelado con Optimización de Hiperparámetros\n",
                "\n",
                "Este script realiza la optimización de hiperparámetros usando Optuna para múltiples modelos y métricas, selecciona el mejor modelo y lo guarda para producción."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# -*- coding: utf-8 -*-\n",
                "\n",
                "import os\n",
                "import warnings\n",
                "import joblib\n",
                "import optuna\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.metrics import f1_score, recall_score, matthews_corrcoef\n",
                "from sklearn.model_selection import StratifiedKFold\n",
                "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, HistGradientBoostingClassifier\n",
                "from xgboost import XGBClassifier\n",
                "from lightgbm import LGBMClassifier\n",
                "from catboost import CatBoostClassifier\n",
                "from imblearn.over_sampling import SMOTE\n",
                "\n",
                "# Importar configuración y utilidades compartidas\n",
                "from config import (\n",
                "    CSV_ENTRENAR, CSV_VALIDAR, MODELO_PRELIMINAR, MODELO_FINAL,\n",
                "    RANDOM_STATE, THRESHOLDS, N_TRIALS_OPTUNA, MODELOS_DISPONIBLES,\n",
                "    UMBRAL_CICLOS_DEFAULT, PESO_FALSOS_POSITIVOS, METRICAS,\n",
                "    configurar_logging\n",
                ")\n",
                "from utils import crear_derivadas, calcular_score_balanceado, evaluar_con_umbral\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
                "\n",
                "# Configurar logger\n",
                "logger = configurar_logging(__name__)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Carga del Umbral Óptimo desde Modelo Preliminar"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ================================================================================\n",
                "# CARGA DEL UMBRAL DESDE MODELO PRELIMINAR\n",
                "# ================================================================================\n",
                "\n",
                "# Intentar cargar el umbral óptimo del modelo preliminar\n",
                "if os.path.exists(MODELO_PRELIMINAR):\n",
                "    config_preliminar = joblib.load(MODELO_PRELIMINAR)\n",
                "    UMBRAL_OPTIMO = config_preliminar.get('umbral_ciclos', UMBRAL_CICLOS_DEFAULT)\n",
                "    logger.info(f\"Umbral cargado desde modelo preliminar: {UMBRAL_OPTIMO}\")\n",
                "else:\n",
                "    UMBRAL_OPTIMO = UMBRAL_CICLOS_DEFAULT\n",
                "    logger.warning(f\"Modelo preliminar no encontrado. Usando umbral por defecto: {UMBRAL_OPTIMO}\")\n",
                "\n",
                "# Modelos base a optimizar\n",
                "MODELOS_BASE = MODELOS_DISPONIBLES\n",
                "\n",
                "# Métricas a evaluar\n",
                "METRICA_1 = 'f1'\n",
                "METRICA_2 = 'recall'\n",
                "METRICA_3 = 'mcc'\n",
                "METRICA_PERSONALIZADA = 'Score Propio'"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Funciones de Optimización con Optuna"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ================================================================================\n",
                "# FUNCIONES DE OPTIMIZACIÓN CON OPTUNA\n",
                "# ================================================================================\n",
                "\n",
                "def obtener_parametros_modelo(trial, model_name):\n",
                "    \"\"\"\n",
                "    Obtiene los parámetros a optimizar para cada modelo.\n",
                "    \n",
                "    Args:\n",
                "        trial: Objeto trial de Optuna\n",
                "        model_name: Nombre del modelo\n",
                "        \n",
                "    Returns:\n",
                "        Tupla (params, model_class)\n",
                "    \"\"\"\n",
                "    if model_name == 'XGBoost':\n",
                "        params = {\n",
                "            'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
                "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
                "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
                "            'scale_pos_weight': trial.suggest_int('scale_pos_weight', 5, 30),\n",
                "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
                "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
                "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
                "            'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),\n",
                "            'random_state': RANDOM_STATE,\n",
                "            'eval_metric': 'aucpr',\n",
                "            'n_jobs': -1\n",
                "        }\n",
                "        return params, XGBClassifier\n",
                "    \n",
                "    elif model_name == 'LightGBM':\n",
                "        params = {\n",
                "            'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
                "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
                "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
                "            'min_child_samples': trial.suggest_int('min_child_samples', 3, 20),\n",
                "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
                "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
                "            'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),\n",
                "            'class_weight': 'balanced',\n",
                "            'random_state': RANDOM_STATE,\n",
                "            'verbose': -1,\n",
                "            'n_jobs': -1\n",
                "        }\n",
                "        return params, LGBMClassifier\n",
                "    \n",
                "    elif model_name == 'RandomForest':\n",
                "        params = {\n",
                "            'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
                "            'max_depth': trial.suggest_int('max_depth', 5, 15),\n",
                "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
                "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 15),\n",
                "            'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
                "            'class_weight': 'balanced_subsample',\n",
                "            'random_state': RANDOM_STATE,\n",
                "            'n_jobs': -1\n",
                "        }\n",
                "        return params, RandomForestClassifier\n",
                "    \n",
                "    elif model_name == 'ExtraTrees':\n",
                "        params = {\n",
                "            'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
                "            'max_depth': trial.suggest_int('max_depth', 5, 20),\n",
                "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
                "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 15),\n",
                "            'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
                "            'class_weight': 'balanced_subsample',\n",
                "            'random_state': RANDOM_STATE,\n",
                "            'n_jobs': -1\n",
                "        }\n",
                "        return params, ExtraTreesClassifier\n",
                "    \n",
                "    elif model_name == 'HistGradientBoosting':\n",
                "        params = {\n",
                "            'max_iter': trial.suggest_int('max_iter', 100, 500),\n",
                "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
                "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
                "            'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 15, 63),\n",
                "            'l2_regularization': trial.suggest_float('l2_regularization', 1e-6, 1.0, log=True),\n",
                "            'class_weight': 'balanced',\n",
                "            'random_state': RANDOM_STATE\n",
                "        }\n",
                "        return params, HistGradientBoostingClassifier\n",
                "    \n",
                "    elif model_name == 'CatBoost':\n",
                "        params = {\n",
                "            'iterations': trial.suggest_int('iterations', 100, 500),\n",
                "            'depth': trial.suggest_int('depth', 3, 10),\n",
                "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
                "            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 10.0, log=True),\n",
                "            'allow_writing_files': False,\n",
                "            'auto_class_weights': 'Balanced',\n",
                "            'random_state': RANDOM_STATE,\n",
                "            'verbose': 0,\n",
                "            'thread_count': -1\n",
                "        }\n",
                "        return params, CatBoostClassifier"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def optimizar_modelo_balanceado(model_name, X_train_full, y_train_full, y_ciclos_full, umbral, cv_splits=5):\n",
                "    \"\"\"\n",
                "    Crea función objetivo para optimizar usando el score personalizado.\n",
                "    \n",
                "    Args:\n",
                "        model_name: Nombre del modelo\n",
                "        X_train_full: Features de entrenamiento\n",
                "        y_train_full: Labels de entrenamiento\n",
                "        y_ciclos_full: Ciclos de entrenamiento\n",
                "        umbral: Umbral de ciclos\n",
                "        cv_splits: Número de folds para CV\n",
                "        \n",
                "    Returns:\n",
                "        Función objetivo para Optuna\n",
                "    \"\"\"\n",
                "    def objetivo(trial):\n",
                "        params, model_class = obtener_parametros_modelo(trial, model_name)\n",
                "        \n",
                "        scores_cv = []\n",
                "        cv = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=RANDOM_STATE)\n",
                "        X_val_np = X_train_full.to_numpy() if isinstance(X_train_full, pd.DataFrame) else X_train_full\n",
                "        y_val_np = y_train_full\n",
                "        columnas = X_train_full.columns if isinstance(X_train_full, pd.DataFrame) else None\n",
                "\n",
                "        for train_idx, val_idx in cv.split(X_train_full, y_train_full):\n",
                "            X_fold_train = X_val_np[train_idx]\n",
                "            y_fold_train = y_val_np[train_idx]\n",
                "            X_fold_val = X_val_np[val_idx]\n",
                "            y_fold_val = y_val_np[val_idx]\n",
                "            y_fold_ciclos = y_ciclos_full[val_idx]\n",
                "\n",
                "            # Aplicar SMOTE\n",
                "            k_neighbors = min(3, y_fold_train.sum() - 1)\n",
                "            smote = SMOTE(random_state=RANDOM_STATE, k_neighbors=k_neighbors)\n",
                "\n",
                "            try:\n",
                "                X_fold_train_bal, y_fold_train_bal = smote.fit_resample(X_fold_train, y_fold_train)\n",
                "            except ValueError:\n",
                "                X_fold_train_bal, y_fold_train_bal = X_fold_train, y_fold_train\n",
                "\n",
                "            model = model_class(**params)\n",
                "\n",
                "            if columnas is not None:\n",
                "                X_fold_train_bal = pd.DataFrame(X_fold_train_bal, columns=columnas)\n",
                "                X_fold_val_df = pd.DataFrame(X_fold_val, columns=columnas)\n",
                "            else:\n",
                "                X_fold_val_df = X_fold_val\n",
                "\n",
                "            model.fit(X_fold_train_bal, y_fold_train_bal)\n",
                "            probas_val = model.predict_proba(X_fold_val_df)[:, 1]\n",
                "            \n",
                "            # Buscar mejor threshold\n",
                "            mejor_score_fold = float('-inf')\n",
                "            thresholds_opt = np.arange(0.1, 0.91, 0.1)\n",
                "\n",
                "            for th in thresholds_opt:\n",
                "                res = evaluar_con_umbral(y_fold_val, probas_val, y_fold_ciclos, umbral, th)\n",
                "                if res:\n",
                "                    sc = calcular_score_balanceado(res['pct_membranas'], res['FP'], res['membranas_total'], PESO_FALSOS_POSITIVOS)\n",
                "                    if sc > mejor_score_fold:\n",
                "                        mejor_score_fold = sc\n",
                "\n",
                "            scores_cv.append(mejor_score_fold if mejor_score_fold > float('-inf') else -100)\n",
                "\n",
                "        return np.mean(scores_cv)\n",
                "\n",
                "    return objetivo\n",
                "\n",
                "\n",
                "def optimizar_modelo_metricas(model_name, X_train_full, y_train_full, metric, cv_splits=5):\n",
                "    \"\"\"\n",
                "    Crea función objetivo para optimizar usando métricas estándar.\n",
                "    \n",
                "    Args:\n",
                "        model_name: Nombre del modelo\n",
                "        X_train_full: Features de entrenamiento\n",
                "        y_train_full: Labels de entrenamiento\n",
                "        metric: Métrica a optimizar ('f1', 'recall', 'mcc')\n",
                "        cv_splits: Número de folds para CV\n",
                "        \n",
                "    Returns:\n",
                "        Función objetivo para Optuna\n",
                "    \"\"\"\n",
                "    def objetivo(trial):\n",
                "        params, model_class = obtener_parametros_modelo(trial, model_name)\n",
                "        \n",
                "        scores_cv = []\n",
                "        cv = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=RANDOM_STATE)\n",
                "        X_val_np = X_train_full.to_numpy() if isinstance(X_train_full, pd.DataFrame) else X_train_full\n",
                "        y_val_np = y_train_full\n",
                "        columnas_f = X_train_full.columns if isinstance(X_train_full, pd.DataFrame) else None\n",
                "\n",
                "        for train_idx, val_idx in cv.split(X_train_full, y_train_full):\n",
                "            X_fold_train, y_fold_train = X_val_np[train_idx], y_val_np[train_idx]\n",
                "            X_fold_val, y_fold_val = X_val_np[val_idx], y_val_np[val_idx]\n",
                "            \n",
                "            # Aplicar SMOTE\n",
                "            k_n = min(3, y_fold_train.sum() - 1)\n",
                "            smote = SMOTE(random_state=RANDOM_STATE, k_neighbors=k_n)\n",
                "\n",
                "            try:\n",
                "                X_fold_train_bal, y_fold_train_bal = smote.fit_resample(X_fold_train, y_fold_train)\n",
                "            except ValueError:\n",
                "                X_fold_train_bal, y_fold_train_bal = X_fold_train, y_fold_train\n",
                "\n",
                "            model = model_class(**params)\n",
                "\n",
                "            if columnas_f is not None:\n",
                "                X_fold_train_bal = pd.DataFrame(X_fold_train_bal, columns=columnas_f)\n",
                "                X_fold_val_df = pd.DataFrame(X_fold_val, columns=columnas_f)\n",
                "            else:\n",
                "                X_fold_val_df = X_fold_val\n",
                "\n",
                "            model.fit(X_fold_train_bal, y_fold_train_bal)\n",
                "\n",
                "            y_pred = model.predict(X_fold_val_df)\n",
                "            \n",
                "            if metric == 'f1':\n",
                "                score = f1_score(y_fold_val, y_pred)\n",
                "            elif metric == 'recall':\n",
                "                score = recall_score(y_fold_val, y_pred)\n",
                "            elif metric == 'mcc':\n",
                "                score = matthews_corrcoef(y_fold_val, y_pred)\n",
                "            else:\n",
                "                score = 0\n",
                "\n",
                "            scores_cv.append(score)\n",
                "\n",
                "        return np.mean(scores_cv)\n",
                "\n",
                "    return objetivo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def crear_modelo(model_name, best_params):\n",
                "    \"\"\"\n",
                "    Crea un modelo con los mejores parámetros encontrados.\n",
                "    \n",
                "    Args:\n",
                "        model_name: Nombre del modelo\n",
                "        best_params: Diccionario con los mejores parámetros\n",
                "        \n",
                "    Returns:\n",
                "        Instancia del modelo configurado\n",
                "    \"\"\"\n",
                "    if model_name == 'XGBoost':\n",
                "        return XGBClassifier(**best_params, random_state=RANDOM_STATE, eval_metric='aucpr', n_jobs=-1)\n",
                "    elif model_name == 'LightGBM':\n",
                "        return LGBMClassifier(**best_params, class_weight='balanced', random_state=RANDOM_STATE, verbose=-1, n_jobs=-1)\n",
                "    elif model_name == 'RandomForest':\n",
                "        return RandomForestClassifier(**best_params, class_weight='balanced_subsample', random_state=RANDOM_STATE, n_jobs=-1)\n",
                "    elif model_name == 'ExtraTrees':\n",
                "        return ExtraTreesClassifier(**best_params, class_weight='balanced_subsample', random_state=RANDOM_STATE, n_jobs=-1)\n",
                "    elif model_name == 'HistGradientBoosting':\n",
                "        return HistGradientBoostingClassifier(**best_params, class_weight='balanced', random_state=RANDOM_STATE)\n",
                "    elif model_name == 'CatBoost':\n",
                "        return CatBoostClassifier(**best_params, allow_writing_files=False, auto_class_weights='Balanced', \n",
                "                                   random_state=RANDOM_STATE, verbose=0, thread_count=-1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Carga y Preparación de Datos"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ================================================================================\n",
                "# CARGA Y PREPARACIÓN DE DATOS\n",
                "# ================================================================================\n",
                "\n",
                "logger.info(\"Cargando datos...\")\n",
                "\n",
                "df_train = pd.read_csv(CSV_ENTRENAR)\n",
                "df_test = pd.read_csv(CSV_VALIDAR)\n",
                "\n",
                "# Eliminar columna de índice\n",
                "for df in [df_train, df_test]:\n",
                "    if 'Indice' in df.columns:\n",
                "        df.drop(columns=['Indice'], inplace=True)\n",
                "\n",
                "# Aplicar ingeniería de características\n",
                "df_train_der = crear_derivadas(df_train)\n",
                "df_test_der = crear_derivadas(df_test)\n",
                "\n",
                "# Separar variables\n",
                "columnas = [c for c in df_train_der.columns if c != 'Ciclos']\n",
                "\n",
                "X_train = df_train_der[columnas]\n",
                "X_test = df_test_der[columnas]\n",
                "y_train_ciclos = df_train['Ciclos'].values\n",
                "y_test_ciclos = df_test['Ciclos'].values\n",
                "\n",
                "# Escalar variables (scaler solo de entrenamiento)\n",
                "scaler = StandardScaler()\n",
                "X_train_esc = pd.DataFrame(scaler.fit_transform(X_train), columns=columnas)\n",
                "X_test_esc = pd.DataFrame(scaler.transform(X_test), columns=columnas)\n",
                "\n",
                "# Aplicar umbral y SMOTE\n",
                "y_train_umbral = (y_train_ciclos < UMBRAL_OPTIMO).astype(int)\n",
                "y_test_umbral = (y_test_ciclos < UMBRAL_OPTIMO).astype(int)\n",
                "\n",
                "k_neighbors = min(3, y_train_umbral.sum() - 1)\n",
                "smote_final = SMOTE(random_state=RANDOM_STATE, k_neighbors=k_neighbors)\n",
                "\n",
                "try:\n",
                "    X_train_res_final, y_train_bal_final = smote_final.fit_resample(X_train_esc, y_train_umbral)\n",
                "    X_train_bal_final = pd.DataFrame(X_train_res_final, columns=columnas)\n",
                "except ValueError as e:\n",
                "    logger.warning(f\"SMOTE falló: {e}\")\n",
                "    X_train_bal_final, y_train_bal_final = X_train_esc, y_train_umbral\n",
                "\n",
                "logger.info(f\"Datos cargados: {len(df_train)} train, {len(df_test)} test\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Optimización de Hiperparámetros"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ================================================================================\n",
                "# FUNCIÓN DE ORQUESTACIÓN\n",
                "# ================================================================================\n",
                "\n",
                "def correr_optimizacion(tipo, metrica_nombre):\n",
                "    \"\"\"\n",
                "    Orquesta la optimización para todos los modelos.\n",
                "    \n",
                "    Args:\n",
                "        tipo: 'balanceado' o 'estandar'\n",
                "        metrica_nombre: Nombre de la métrica\n",
                "        \n",
                "    Returns:\n",
                "        Diccionario con resultados por modelo\n",
                "    \"\"\"\n",
                "    res_modelos = {}\n",
                "    print(f\"\\nOptimizando para métrica: {metrica_nombre}\")\n",
                "\n",
                "    for nombre in MODELOS_BASE:\n",
                "        print(f\"  {nombre}...\", end=\" \")\n",
                "\n",
                "        if tipo == 'balanceado':\n",
                "            objetivo = optimizar_modelo_balanceado(nombre, X_train_esc, y_train_umbral, y_train_ciclos, UMBRAL_OPTIMO, cv_splits=5)\n",
                "        else:\n",
                "            objetivo = optimizar_modelo_metricas(nombre, X_train_esc, y_train_umbral, metrica_nombre, cv_splits=5)\n",
                "\n",
                "        estudio = optuna.create_study(\n",
                "            direction='maximize',\n",
                "            sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE)\n",
                "        )\n",
                "        estudio.optimize(objetivo, n_trials=N_TRIALS_OPTUNA, show_progress_bar=False)\n",
                "\n",
                "        mejores_parametros = estudio.best_params\n",
                "        mejor_score_cv = estudio.best_value\n",
                "        mejor_modelo = crear_modelo(nombre, mejores_parametros)\n",
                "        mejor_modelo.fit(X_train_bal_final, y_train_bal_final)\n",
                "\n",
                "        res_modelos[nombre] = {\n",
                "            'modelo': mejor_modelo,\n",
                "            'parametros': mejores_parametros,\n",
                "            'cv_score': mejor_score_cv\n",
                "        }\n",
                "\n",
                "        print(f\"Score: {mejor_score_cv:.4f}\")\n",
                "        \n",
                "    return res_modelos"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ================================================================================\n",
                "# EJECUTAR OPTIMIZACIÓN\n",
                "# ================================================================================\n",
                "\n",
                "logger.info(\"Iniciando exploración de hiperparámetros...\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "modelos_1 = correr_optimizacion('estandar', METRICA_1)\n",
                "modelos_2 = correr_optimizacion('estandar', METRICA_2)\n",
                "modelos_3 = correr_optimizacion('estandar', METRICA_3)\n",
                "modelos_pers = correr_optimizacion('balanceado', METRICA_PERSONALIZADA)\n",
                "\n",
                "# Consolidar resultados\n",
                "mejores_modelos = {\n",
                "    **{f\"{k}_{METRICA_1}\": v for k, v in modelos_1.items()},\n",
                "    **{f\"{k}_{METRICA_2}\": v for k, v in modelos_2.items()},\n",
                "    **{f\"{k}_{METRICA_3}\": v for k, v in modelos_3.items()},\n",
                "    **{f\"{k}_{METRICA_PERSONALIZADA}\": v for k, v in modelos_pers.items()}\n",
                "}\n",
                "\n",
                "print(\"\\nExploración terminada.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluación de Resultados"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ================================================================================\n",
                "# EVALUACIÓN EN TEST\n",
                "# ================================================================================\n",
                "\n",
                "print(\"Resultados en conjunto de validación:\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "resultados_optimizados = []\n",
                "probas_test = {}\n",
                "mejor_config_global = None\n",
                "mejor_score_global = float('-inf')\n",
                "mejor_nombre_global = None\n",
                "\n",
                "for nombre, info in mejores_modelos.items():\n",
                "    clf = info['modelo']\n",
                "    y_proba = clf.predict_proba(X_test_esc)[:, 1]\n",
                "    probas_test[nombre] = y_proba\n",
                "    mejor_config = None\n",
                "    mejor_score = float('-inf')\n",
                "\n",
                "    for th in THRESHOLDS:\n",
                "        resultado_threshold = evaluar_con_umbral(y_test_umbral, y_proba, y_test_ciclos, UMBRAL_OPTIMO, th)\n",
                "        if resultado_threshold:\n",
                "            resultado_threshold['modelo'] = nombre\n",
                "            resultados_optimizados.append(resultado_threshold)\n",
                "\n",
                "            score = calcular_score_balanceado(\n",
                "                resultado_threshold['pct_membranas'], \n",
                "                resultado_threshold['FP'], \n",
                "                resultado_threshold['membranas_total'],\n",
                "                PESO_FALSOS_POSITIVOS\n",
                "            )\n",
                "            if score > mejor_score:\n",
                "                mejor_score = score\n",
                "                mejor_config = resultado_threshold\n",
                "                mejor_config['score_balanceado'] = score\n",
                "\n",
                "    if mejor_config:\n",
                "        print(f\"{nombre}: th={mejor_config['threshold']:.2f} | \"\n",
                "              f\"Membranas={mejor_config['membranas_detectadas']}/{mejor_config['membranas_total']} | \"\n",
                "              f\"FP={mejor_config['FP']} | Score={mejor_config['score_balanceado']:.2f}\")\n",
                "        \n",
                "        # Guardar el mejor global\n",
                "        if mejor_score > mejor_score_global:\n",
                "            mejor_score_global = mejor_score\n",
                "            mejor_config_global = mejor_config\n",
                "            mejor_nombre_global = nombre"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Entrenamiento del Modelo Final"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ================================================================================\n",
                "# ENTRENAMIENTO CON TODOS LOS DATOS\n",
                "# ================================================================================\n",
                "\n",
                "print(f\"\\nMejor modelo: {mejor_nombre_global} (Score: {mejor_score_global:.2f})\")\n",
                "print(\"Entrenando modelo final con todos los datos...\")\n",
                "\n",
                "# Obtener parámetros del mejor modelo\n",
                "params_ganadores = mejores_modelos[mejor_nombre_global]['parametros']\n",
                "nombre_modelo_base = mejor_nombre_global.split('_')[0]  # Extraer nombre base (ej: 'ExtraTrees')\n",
                "\n",
                "# Unir datasets\n",
                "X_full = pd.concat([X_train, X_test], axis=0).reset_index(drop=True)\n",
                "y_full_ciclos = np.concatenate([y_train_ciclos, y_test_ciclos])\n",
                "y_full_umbral = (y_full_ciclos < UMBRAL_OPTIMO).astype(int)\n",
                "\n",
                "# Reescalar con SOLO datos de entrenamiento (scaler ya ajustado)\n",
                "# NOTA: Usamos el scaler original para mantener consistencia\n",
                "X_full_esc = pd.DataFrame(scaler.transform(X_full), columns=columnas)\n",
                "\n",
                "# SMOTE\n",
                "k_neighbors = min(3, y_full_umbral.sum() - 1)\n",
                "smote_full = SMOTE(random_state=RANDOM_STATE, k_neighbors=k_neighbors)\n",
                "\n",
                "try:\n",
                "    X_full_bal, y_full_bal = smote_full.fit_resample(X_full_esc, y_full_umbral)\n",
                "    X_full_bal = pd.DataFrame(X_full_bal, columns=columnas)\n",
                "except ValueError as e:\n",
                "    logger.warning(f\"SMOTE falló en entrenamiento final: {e}\")\n",
                "    X_full_bal, y_full_bal = X_full_esc, y_full_umbral\n",
                "\n",
                "# Crear y entrenar modelo final\n",
                "modelo_final = crear_modelo(nombre_modelo_base, params_ganadores)\n",
                "modelo_final.fit(X_full_bal, y_full_bal)\n",
                "\n",
                "logger.info(\"Modelo final entrenado.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Guardado del Modelo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ================================================================================\n",
                "# GUARDADO DEL MODELO\n",
                "# ================================================================================\n",
                "\n",
                "config_produccion = {\n",
                "    'umbral_ciclos': UMBRAL_OPTIMO,\n",
                "    'threshold': mejor_config_global['threshold'],  # Clave consistente (singular)\n",
                "    'scaler': scaler,  # Scaler ajustado solo con datos de entrenamiento\n",
                "    'modelo': modelo_final,        \n",
                "    'feature_cols': columnas,\n",
                "    'nombre_modelo': nombre_modelo_base\n",
                "}\n",
                "\n",
                "# Crear directorio si no existe\n",
                "os.makedirs(os.path.dirname(MODELO_FINAL), exist_ok=True)\n",
                "\n",
                "# Guardar modelo\n",
                "joblib.dump(config_produccion, MODELO_FINAL)\n",
                "\n",
                "logger.info(f\"Modelo guardado en: {MODELO_FINAL}\")\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"MODELADO COMPLETADO\")\n",
                "print(\"=\"*50)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.21"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}